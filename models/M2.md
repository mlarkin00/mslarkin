Below is a minimal, runnable Golang app that serves a simple web UI to select from a list of models and chat with the selected model via Google’s Generative Language API (Gemini). It streams responses using Server-Sent Events (SSE). You only need a Google AI API key.

Project structure:

- go.mod
- main.go

go.mod:

```
module mslarkin-ext

go 1.22

require (
	github.com/google/uuid v1.6.0
)
```

main.go:

```
package main

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"log"
	"net/http"
	"os"
	"strings"
	"time"

	"github.com/google/uuid"
)

const (
	addr              = ":8080"
	googleAPIKeyEnv  = "GOOGLE_AI_API_KEY"
	googleAPIBaseURL = "https://generativelanguage.googleapis.com/v1beta"
)

var allowedModels = []string{
	"qwen/qwen3-next-80b-a3b-thinking-maas",
	"qwen/qwen3-coder-480b-a35b-instruct-maas",
	"publishers/zai-org/models/glm-4.7:GLM-4.7-FP8",
	"publishers/minimaxai/models/minimax-m2-maas",
}

type Message struct {
	Role    string `json:"role"` // "user" or "model"
	Content string `json:"content"`
}

type ChatRequest struct {
	Model   string    `json:"model"`
	History []Message `json:"history"`
}

type GLPart struct {
	Text string `json:"text,omitempty"`
}

type GLContent struct {
	Role string   `json:"role"` // "user" or "model"
	Parts []GLPart `json:"parts"`
}

type GLGenerateBody struct {
	Contents         []GLContent `json:"contents"`
	GenerationConfig struct {
		Temperature     float64 `json:"temperature,omitempty"`
		TopP            float64 `json:"top_p,omitempty"`
		MaxOutputTokens int      `json:"max_output_tokens,omitempty"`
		StopSequences   []string `json:"stop_sequences,omitempty"`
	} `json:"generationConfig,omitempty"`
}

type GLStreamCandidate struct {
	Content *GLContent `json:"content,omitempty"`
}

type GLStreamResponse struct {
	Candidates []GLStreamCandidate `json:"candidates,omitempty"`
}

func main() {
	apiKey := os.Getenv(googleAPIKeyEnv)
	if apiKey == "" {
		log.Fatalf("Environment variable %s is not set", googleAPIKeyEnv)
	}

	mux := http.NewServeMux()
	mux.HandleFunc("/", handleIndex)
	mux.HandleFunc("/models", handleModels)
	mux.HandleFunc("/stream", handleStreamChat(apiKey))

	srv := &http.Server{
		Addr:              addr,
		Handler:           cors(mux),
		ReadHeaderTimeout: 10 * time.Second,
	}

	log.Printf("mslarkin-ext server listening on %s", addr)
	if err := srv.ListenAndServe(); err != nil && err != http.ErrServerClosed {
		log.Fatalf("Server error: %v", err)
	}
}

func cors(h http.Handler) http.Handler {
	return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		// Basic CORS for local dev; adjust as needed for production
		w.Header().Set("Access-Control-Allow-Origin", "*")
		w.Header().Set("Access-Control-Allow-Methods", "GET, POST, OPTIONS")
		w.Header().Set("Access-Control-Allow-Headers", "Content-Type, Accept")
		if r.Method == http.MethodOptions {
			w.WriteHeader(http.StatusNoContent)
			return
		}
		h.ServeHTTP(w, r)
	})
}

func handleIndex(w http.ResponseWriter, r *http.Request) {
	if r.URL.Path != "/" {
		http.NotFound(w, r)
		return
	}
	w.Header().Set("Content-Type", "text/html; charset=utf-8")
	fmt.Fprint(w, indexHTML)
}

func handleModels(w http.ResponseWriter, r *http.Request) {
	w.Header().Set("Content-Type", "application/json")
	_ = json.NewEncoder(w).Encode(map[string]interface{}{
		"models": allowedModels,
	})
}

func handleStreamChat(apiKey string) http.HandlerFunc {
	return func(w http.ResponseWriter, r *http.Request) {
		if r.Method != http.MethodPost {
			http.Error(w, "method not allowed", http.StatusMethodNotAllowed)
			return
		}
		var req ChatRequest
		if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
			http.Error(w, "invalid JSON", http.StatusBadRequest)
			return
		}
		if !isAllowedModel(req.Model) {
			http.Error(w, "model not allowed", http.StatusBadRequest)
			return
		}

		ctx := r.Context()
		flusher, ok := w.(http.Flusher)
		if !ok {
			http.Error(w, "streaming unsupported", http.StatusInternalServerError)
			return
		}

		// Prepare SSE
		w.Header().Set("Content-Type", "text/event-stream")
		w.Header().Set("Cache-Control", "no-cache")
		w.Header().Set("Connection", "keep-alive")
		// Some reverse proxies buffer; disable buffering if possible
		w.Header().Set("X-Accel-Buffering", "no")

		// Build GL request body
		glBody := GLGenerateBody{
			Contents: buildContents(req.History),
			GenerationConfig: struct {
				Temperature     float64 `json:"temperature,omitempty"`
				TopP            float64 `json:"top_p,omitempty"`
				MaxOutputTokens int      `json:"max_output_tokens,omitempty"`
				StopSequences   []string `json:"stop_sequences,omitempty"`
			}{
				Temperature:     0.7,
				TopP:            0.9,
				MaxOutputTokens: 1024,
			},
		}
		payload, err := json.Marshal(glBody)
		if err != nil {
			http.Error(w, "failed to marshal request", http.StatusInternalServerError)
			return
		}

		url := fmt.Sprintf("%s/models/%s:streamGenerateContent?key=%s", googleAPIKeyURLPart(), req.Model, apiKey)
		hc := &http.Client{Timeout: 0} // no explicit timeout; rely on server or context
		gReq, err := http.NewRequestWithContext(ctx, "POST", url, bytes.NewReader(payload))
		if err != nil {
			http.Error(w, "failed to create upstream request", http.StatusInternalServerError)
			return
		}
		gReq.Header.Set("Content-Type", "application/json")

		resp, err := hc.Do(gReq)
		if err != nil {
			sendSSE(w, flusher, "error", fmt.Sprintf("upstream error: %v", err))
			return
		}
		defer resp.Body.Close()
		if resp.StatusCode >= 300 {
			b, _ := io.ReadAll(resp.Body)
			sendSSE(w, flusher, "error", fmt.Sprintf("upstream status %d: %s", resp.StatusCode, string(b)))
			return
		}

		// Stream SSE to client
		dec := json.NewDecoder(resp.Body)
		for {
			var gr GLStreamResponse
			if err := dec.Decode(&gr); err != nil {
				if ctx.Err() != nil {
					// client canceled
					return
				}
				// JSON decode may fail on keep-alive newlines; ignore and continue
				// But if EOF, exit gracefully
				if err == io.EOF {
					break
				}
				// Send a final error event and break
				sendSSE(w, flusher, "error", fmt.Sprintf("decode error: %v", err))
				break
			}
			if len(gr.Candidates) > 0 && gr.Candidates[0].Content != nil {
				parts := gr.Candidates[0].Content.Parts
				if len(parts) > 0 && parts[0].Text != "" {
					sendSSE(w, flusher, "message", parts[0].Text)
				}
			}
		}
		// Signal end
		sendSSE(w, flusher, "done", "")
	}
}

func isAllowedModel(m string) bool {
	for _, allowed := range allowedModels {
		if m == allowed {
			return true
		}
	}
	return false
}

func buildContents(history []Message) []GLContent {
	// Gemini API expects role to be "user" or "model"
	// We map user->user and model->model
	contents := make([]GLContent, 0, len(history))
	for _, m := range history {
		role := strings.ToLower(m.Role)
		if role != "user" && role != "model" {
			role = "user"
		}
		contents = append(contents, GLContent{
			Role: role,
			Parts: []GLPart{{Text: m.Content}},
		})
	}
	return contents
}

func googleAPIKeyURLPart() string {
	return googleAPIBaseURL
}

func sendSSE(w http.ResponseWriter, flusher http.Flusher, event, data string) {
	// id is optional; we include a UUID for uniqueness
	id := uuid.New().String()
	fmt.Fprintf(w, "id: %s\nevent: %s\ndata: %s\n\n", id, event, data)
	flusher.Flush()
}

const indexHTML = `<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>mslarkin-ext - Model Chat</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style>
    :root {
      --bg: #0f172a;
      --panel: #111827;
      --text: #e5e7eb;
      --muted: #9ca3af;
      --accent: #3b82f6;
      --accent-2: #22d3ee;
      --danger: #ef4444;
      --ok: #10b981;
    }
    * { box-sizing: border-box; }
    body {
      margin: 0; background: linear-gradient(135deg, #0b1020, #0f172a);
      color: var(--text); font-family: Inter, system-ui, -apple-system, Segoe UI, Roboto, Ubuntu, Cantarell, Noto Sans, Helvetica, Arial, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
      min-height: 100vh; display: flex; align-items: center; justify-content: center;
    }
    .app {
      width: min(1100px, 95vw); height: min(90vh, 900px);
      background: rgba(17, 24, 39, 0.75); backdrop-filter: blur(8px);
      border: 1px solid rgba(255,255,255,0.08);
      border-radius: 16px; display: grid; grid-template-rows: auto 1fr auto;
      overflow: hidden; box-shadow: 0 10px 40px rgba(0,0,0,0.5);
    }
    header {
      padding: 14px 16px; display: flex; align-items: center; gap: 12px;
      border-bottom: 1px solid rgba(255,255,255,0.08);
      background: linear-gradient(180deg, rgba(255,255,255,0.04), transparent);
    }
    header h1 { font-size: 16px; margin: 0; font-weight: 600; letter-spacing: 0.3px; }
    .spacer { flex: 1; }
    select, button {
      background: #0b1220; color: var(--text);
      border: 1px solid rgba(255,255,255,0.12);
      border-radius: 10px; padding: 10px 12px; font-size: 14px;
    }
    button {
      background: linear-gradient(135deg, var(--accent), var(--accent-2));
      border: none; color: white; font-weight: 600; cursor: pointer;
      transition: transform 0.05s ease, box-shadow 0.2s ease;
      box-shadow: 0 6px 18px rgba(59,130,246,0.35);
    }
    button:disabled { opacity: 0.6; cursor: not-allowed; box-shadow: none; }
    button:active { transform: translateY(1px); }
    .chat {
      padding: 16px; overflow-y: auto; display: flex; flex-direction: column; gap: 12px;
    }
    .msg {
      max-width: 85%; padding: 12px 14px; border-radius: 12px; line-height: 1.45;
      white-space: pre-wrap; word-wrap: break-word; border: 1px solid rgba(255,255,255,0.08);
    }
    .msg.user { align-self: flex-end; background: rgba(59,130,246,0.12); }
    .msg.assistant { align-self: flex-start; background: rgba(34,211,238,0.12); }
    .msg.error { align-self: flex-start; background: rgba(239,68,68,0.12); border-color: rgba(239,68,68,0.35); }
    .footer {
      padding: 12px; border-top: 1px solid rgba(255,255,255,0.08);
      display: grid; grid-template-columns: 1fr auto; gap: 8px; background: rgba(0,0,0,0.15);
    }
    textarea {
      resize: none; background: #0b1220; color: var(--text);
      border: 1px solid rgba(255,255,255,0.12); border-radius: 10px;
      padding: 10px 12px; font-size: 14px; min-height: 44px; max-height: 180px;
    }
    .meta { font-size: 12px; color: var(--muted); padding: 0 16px 8px; }
    .pill {
      font-size: 11px; color: white; background: rgba(255,255,255,0.12);
      padding: 2px 8px; border-radius: 999px; border: 1px solid rgba(255,255,255,0.14);
    }
    .row { display: flex; align-items: center; gap: 8px; }
    .right { margin-left: auto; }
    .hint { font-size: 12px; color: var(--muted); }
  </style>
</head>
<body>
  <div class="app">
    <header>
      <h1>mslarkin-ext</h1>
      <span class="pill">Google Generative Language API</span>
      <div class="spacer"></div>
      <div class="row">
        <label for="model" class="hint">Model</label>
        <select id="model"></select>
      </div>
    </header>

    <div class="meta">
      <span id="status">Ready</span>
      <span class="right hint">Shift+Enter for newline • Enter to send</span>
    </div>

    <div id="chat" class="chat"></div>

    <div class="footer">
      <textarea id="input" placeholder="Type your message..."></textarea>
      <button id="send">Send</button>
    </div>
  </div>

  <script>
    const chatEl = document.getElementById('chat');
    const inputEl = document.getElementById('input');
    const sendBtn = document.getElementById('send');
    const modelEl = document.getElementById('model');
    const statusEl = document.getElementById('status');

    let history = []; // { role: 'user'|'model', content: string }
    let streaming = false;
    let es = null;

    async function loadModels() {
      try {
        const res = await fetch('/models');
        const data = await res.json();
        modelEl.innerHTML = '';
        (data.models || []).forEach(m => {
          const opt = document.createElement('option');
          opt.value = m; opt.textContent = m;
          modelEl.appendChild(opt);
        });
      } catch (e) {
        console.error(e);
        setStatus('Failed to load models', true);
      }
    }

    function setStatus(text, isError = false) {
      statusEl.textContent = text;
      statusEl.style.color = isError ? 'var(--danger)' : 'var(--muted)';
    }

    function addMessage(role, content, isError = false) {
      const div = document.createElement('div');
      div.className = 'msg ' + (isError ? 'error' : role);
      div.textContent = content;
      chatEl.appendChild(div);
      chatEl.scrollTop = chatEl.scrollHeight;
      return div;
    }

    function setInputEnabled(enabled) {
      inputEl.disabled = !enabled;
      sendBtn.disabled = !enabled;
    }

    async function sendMessage() {
      const text = inputEl.value.trim();
      if (!text || streaming) return;

      const model = modelEl.value;
      if (!model) {
        setStatus('Please select a model', true);
        return;
      }

      // Append user message
      addMessage('user', text);
      history.push({ role: 'user', content: text });
      inputEl.value = '';
      setInputEnabled(false);
      setStatus('Thinking...');
      streaming = true;

      // Create assistant placeholder
      const assistantEl = addMessage('assistant', '');

      // Open SSE stream
      es = new EventSource('/stream');
      let fullText = '';

      es.addEventListener('message', (ev) => {
        try {
          const data = JSON.parse(ev.data);
          if (typeof data === 'string') {
            fullText += data;
          } else if (data && data.content) {
            fullText += data.content;
          }
          assistantEl.textContent = fullText;
          chatEl.scrollTop = chatEl.scrollHeight;
        } catch (e) {
          // ignore parse errors; some proxies may send keepalives
        }
      });

      es.addEventListener('error', (ev) => {
        console.error('SSE error', ev);
        assistantEl.classList.add('error');
        assistantEl.textContent = fullText || 'An error occurred.';
        cleanupStream();
      });

      es.addEventListener('done', () => {
        // Save assistant reply
        history.push({ role: 'model', content: fullText });
        cleanupStream();
      });

      // Send POST body after establishing EventSource
      fetch('/stream', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ model, history })
      }).then(async (res) => {
        if (!res.ok) {
          const text = await res.text();
          assistantEl.classList.add('error');
          assistantEl.textContent = 'Error: ' + text;
          cleanupStream();
        }
      }).catch((err) => {
        console.error(err);
        assistantEl.classList.add('error');
        assistantEl.textContent = 'Network error.';
        cleanupStream();
      });
    }

    function cleanupStream() {
      if (es) {
        es.close();
        es = null;
      }
      streaming = false;
      setInputEnabled(true);
      setStatus('Ready');
    }

    inputEl.addEventListener('keydown', (e) => {
      if (e.key === 'Enter' && !e.shiftKey) {
        e.preventDefault();
        sendMessage();
      }
    });
    sendBtn.addEventListener('click', sendMessage);

    // Initialize
    loadModels();
  </script>
</body>
</html>
`
```

How to run:

- Prerequisites:
  - Go 1.22+ installed
  - A Google AI API key with access to the listed models
- Steps:
  - Set the API key environment variable:
    - macOS/Linux: export GOOGLE_AI_API_KEY="YOUR_KEY"
    - Windows (PowerShell): $env:GOOGLE_AI_API_KEY="YOUR_KEY"
  - Start the server:
    - go run .
  - Open http://localhost:8080 in your browser
  - Select a model, type a message, and chat. Responses stream in real time.

Notes:

- Regions: This app uses the global Google Generative Language API endpoint; regional routing is handled by Google. If you want strict regional routing, deploy behind a regional load balancer in us-west1 (preferred) or us-central1 (fallback).
- Models: The dropdown is hardcoded with your provided model list. If a model is not enabled for your key, you’ll see an upstream error in the chat.
- Security: For production, do not expose your API key to the browser. Keep it server-side as implemented, and add authentication and rate limiting as needed.
